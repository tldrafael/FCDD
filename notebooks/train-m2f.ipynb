{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a406b9f7-dd37-4a83-9001-23547905f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/tldrafael/DeepLabV3Plus-Pytorch\n",
    "# !mv DeepLabV3Plus-Pytorch deeplabv3\n",
    "\n",
    "# !pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b808dd8-81af-4316-988c-67dc3757cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import color_palette\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from glob import iglob\n",
    "import re\n",
    "from datetime import datetime\n",
    "import random\n",
    "import cv2\n",
    "from copy import copy\n",
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "from importlib import reload\n",
    "from glob import iglob\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import utils as ut\n",
    "import dataset as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5312abfd-41d7-4ec8-9819-d96f58a434e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Background', 1: 'Animals', 2: 'Asphalt', 3: \"Cat's eyes\", 4: 'Cracks', 5: 'Dirt road', 6: 'Ego', 7: 'Hard sand', 8: 'Markings', 9: 'Obstacles', 10: 'People', 11: 'Potholes', 12: 'Retaining walls', 13: 'Soft sand', 14: 'Speed bump', 15: 'Vehicles', 16: 'Wet sand'}\n",
      "long: 512, crop_size: (256, 352)\n"
     ]
    }
   ],
   "source": [
    "id2label = ds.IDs('mocamba').id2label\n",
    "n_classes = len(id2label)\n",
    "print(id2label)\n",
    "\n",
    "\n",
    "long = 256\n",
    "long = 512\n",
    "# long = 1024\n",
    "# long = 1536\n",
    "# long = 2048\n",
    "\n",
    "f = long/512\n",
    "crop_size = (256*f, 352*f) \n",
    "crop_size = tuple(int(a) for a in crop_size)\n",
    "print(f'long: {long}, crop_size: {crop_size}')\n",
    "\n",
    "T_crop = T.Compose([\n",
    "    T.RandomCrop(size=crop_size),\n",
    "    T.RandomHorizontalFlip(p=.5)\n",
    "])\n",
    "\n",
    "\n",
    "train_annotation = f'../data/tidyv01-long{long}/trainpaths.txt'\n",
    "val_annotation = f'../data/tidyv01-long{long}/valpaths.txt'\n",
    "\n",
    "\n",
    "train_ds = ut.SimpleDataset(annotation_file=train_annotation, transform=T_crop, transform_target=T_crop)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "val_ds = ut.SimpleDataset(annotation_file=val_annotation)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656877bc-0ae8-430a-a958-20f1017b62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=[]\n",
    "for inp_im, inp_label in train_loader:\n",
    "    # tmp.append(inp_label.max())\n",
    "    break\n",
    "\n",
    "# print(np.max(tmp))\n",
    "    \n",
    "lim = 2\n",
    "dummy_im = inp_im.clone()[:lim]\n",
    "dummy_label = inp_label.clone()[:lim]\n",
    "dummy_label.unique()\n",
    "\n",
    "\n",
    "colorizer = ut.TorchColorizer(len(id2label))\n",
    "\n",
    "ims = ut.Normalize.reverse(dummy_im)\n",
    "labels = colorizer(dummy_label)\n",
    "alpha = .2\n",
    "blend = (1-alpha)*ims + alpha*labels\n",
    "\n",
    "tmp = torch.concat([ims, labels, blend], axis=-2)\n",
    "tmp = tmp.moveaxis(0,-2).flatten(-2,-1).permute(1,2,0)\n",
    "tmp = ut.float_to_uint8(tmp.numpy())\n",
    "# Image.fromarray(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636a43a-38ee-46c3-8272-f5821c93f127",
   "metadata": {},
   "source": [
    "# pre-trained m2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd57778-3f65-4547-bf00-56e320201337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/miniconda3/envs/sam2/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of MyMask2Former were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([18, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ut.MyMask2Former.from_pretrained(\n",
    "            \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b858e38-ab00-4b4a-a4c9-2236d0953ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segmentation_map = model(dummy_im.cuda())[0].cpu()\n",
    "\n",
    "colorizer = ut.TorchColorizer(len(id2label))\n",
    "\n",
    "ims = ut.Normalize.reverse(dummy_im)\n",
    "labels = colorizer(segmentation_map)\n",
    "alpha = .2\n",
    "blend = (1-alpha)*ims + alpha*labels\n",
    "\n",
    "tmp = torch.concat([ims, labels, blend], axis=-2)\n",
    "tmp = tmp.moveaxis(0,-2).flatten(-2,-1).permute(1,2,0)\n",
    "tmp = ut.float_to_uint8(tmp.numpy())\n",
    "# Image.fromarray(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76184e2c-e207-4f5c-9dd9-516f2c62ed16",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edf0be7-fdb5-4036-adc2-b4a98db89530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyMask2Former were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([18, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "reload(ut)\n",
    "model = ut.MyMask2Former.from_pretrained(\n",
    "            \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes=True\n",
    ")\n",
    "model.cuda();\n",
    "n_classes = len(id2label)\n",
    "model.n_classes = n_classes\n",
    "# model = torch.compile(model, fullgraph=True)\n",
    "\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "scheduler = ut.WarmupLR(opt, n_warmup_max=1000)\n",
    "warmup_step = 1\n",
    "\n",
    "\n",
    "time_now = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "time_now += f'-PAPER-m2f-tidyv01+long{long}'\n",
    "logdir = os.path.join('../logs', time_now)\n",
    "# logdir = '20250713_2010-PAPER-m2f-tidyv01+long1024'\n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "writer.flush()\n",
    "\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351c98a8-a285-4c96-9afd-1cefd2cb44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = ut.SimpleDataset(annotation_file=val_annotation)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)\n",
    "\n",
    "bs_train = 8\n",
    "train_loader = DataLoader(train_ds, batch_size=bs_train, shuffle=True, pin_memory=True, persistent_workers=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31b5a11-e1c9-4919-978e-c640580df91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load('../logs/20250713_2010-PAPER-m2f-tidyv01+long1024/model.last.pth', weights_only=False)\n",
    "# model.load_state_dict(state_dict['state'])\n",
    "# opt.load_state_dict(state_dict['opt_state'])\n",
    "# miou_best = state_dict['best_miou']\n",
    "# it = state_dict['it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6012f17b-756a-438a-ad8c-f5d98a084d2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 1.00e-07\n",
      "\tmiou-train: 0.01\tmiou-val: 0.01\n",
      "grad-acc: 1\n",
      "lr: 2.00e-07\n",
      "lr: 3.00e-07\n",
      "lr: 4.00e-07\n",
      "lr: 5.00e-07\n",
      "lr: 6.00e-07\n",
      "lr: 6.99e-07\n",
      "lr: 7.99e-07\n",
      "lr: 8.99e-07\n",
      "lr: 9.99e-07\n",
      "lr: 1.10e-06\n",
      "lr: 1.20e-06\n",
      "lr: 1.30e-06\n",
      "lr: 1.40e-06\n",
      "lr: 1.50e-06\n",
      "lr: 1.60e-06\n",
      "lr: 1.70e-06\n",
      "lr: 1.80e-06\n",
      "lr: 1.90e-06\n",
      "lr: 2.00e-06\n",
      "lr: 2.10e-06\n",
      "lr: 2.20e-06\n",
      "lr: 2.30e-06\n",
      "lr: 2.40e-06\n",
      "lr: 2.50e-06\n",
      "lr: 2.60e-06\n",
      "lr: 2.70e-06\n",
      "lr: 2.80e-06\n",
      "lr: 2.90e-06\n",
      "lr: 3.00e-06\n",
      "lr: 3.10e-06\n",
      "lr: 3.20e-06\n",
      "lr: 3.30e-06\n",
      "lr: 3.40e-06\n",
      "lr: 3.50e-06\n",
      "lr: 3.60e-06\n",
      "lr: 3.70e-06\n",
      "lr: 3.80e-06\n",
      "lr: 3.90e-06\n",
      "lr: 4.00e-06\n",
      "lr: 4.10e-06\n",
      "lr: 4.20e-06\n",
      "lr: 4.30e-06\n",
      "lr: 4.40e-06\n",
      "lr: 4.50e-06\n",
      "lr: 4.60e-06\n",
      "lr: 4.70e-06\n",
      "lr: 4.80e-06\n",
      "lr: 4.90e-06\n",
      "lr: 5.00e-06\n",
      "lr: 5.10e-06\n",
      "lr: 5.19e-06\n",
      "lr: 5.29e-06\n",
      "lr: 5.39e-06\n",
      "lr: 5.49e-06\n",
      "lr: 5.59e-06\n",
      "lr: 5.69e-06\n",
      "lr: 5.79e-06\n",
      "lr: 5.89e-06\n",
      "lr: 5.99e-06\n",
      "lr: 6.09e-06\n",
      "lr: 6.19e-06\n",
      "lr: 6.29e-06\n",
      "lr: 6.39e-06\n",
      "lr: 6.49e-06\n",
      "lr: 6.59e-06\n",
      "lr: 6.69e-06\n",
      "lr: 6.79e-06\n",
      "lr: 6.89e-06\n",
      "lr: 6.99e-06\n",
      "lr: 7.09e-06\n",
      "lr: 7.19e-06\n",
      "lr: 7.29e-06\n",
      "lr: 7.39e-06\n",
      "lr: 7.49e-06\n",
      "lr: 7.59e-06\n",
      "lr: 7.69e-06\n",
      "lr: 7.79e-06\n",
      "lr: 7.89e-06\n",
      "lr: 7.99e-06\n",
      "lr: 8.09e-06\n",
      "lr: 8.19e-06\n",
      "lr: 8.29e-06\n",
      "lr: 8.39e-06\n",
      "lr: 8.49e-06\n",
      "lr: 8.59e-06\n",
      "lr: 8.69e-06\n",
      "lr: 8.79e-06\n",
      "lr: 8.89e-06\n",
      "lr: 8.99e-06\n",
      "lr: 9.09e-06\n",
      "lr: 9.19e-06\n",
      "lr: 9.29e-06\n",
      "lr: 9.39e-06\n",
      "lr: 9.49e-06\n",
      "lr: 9.59e-06\n",
      "lr: 9.69e-06\n",
      "lr: 9.79e-06\n",
      "lr: 9.89e-06\n",
      "lr: 9.99e-06\n",
      "lr: 1.01e-05\n",
      "lr: 1.02e-05\n",
      "lr: 1.03e-05\n",
      "lr: 1.04e-05\n",
      "lr: 1.05e-05\n",
      "lr: 1.06e-05\n",
      "lr: 1.07e-05\n",
      "lr: 1.08e-05\n",
      "lr: 1.09e-05\n",
      "lr: 1.10e-05\n",
      "lr: 1.11e-05\n",
      "lr: 1.12e-05\n",
      "lr: 1.13e-05\n",
      "lr: 1.14e-05\n",
      "lr: 1.15e-05\n",
      "lr: 1.16e-05\n",
      "lr: 1.17e-05\n",
      "lr: 1.18e-05\n",
      "lr: 1.19e-05\n",
      "lr: 1.20e-05\n",
      "lr: 1.21e-05\n",
      "lr: 1.22e-05\n",
      "lr: 1.23e-05\n",
      "lr: 1.24e-05\n",
      "lr: 1.25e-05\n",
      "lr: 1.26e-05\n",
      "lr: 1.27e-05\n",
      "lr: 1.28e-05\n",
      "lr: 1.29e-05\n",
      "lr: 1.30e-05\n",
      "lr: 1.31e-05\n",
      "lr: 1.32e-05\n",
      "lr: 1.33e-05\n",
      "lr: 1.34e-05\n",
      "lr: 1.35e-05\n",
      "lr: 1.36e-05\n",
      "lr: 1.37e-05\n",
      "lr: 1.38e-05\n",
      "lr: 1.39e-05\n",
      "lr: 1.40e-05\n",
      "lr: 1.41e-05\n",
      "lr: 1.42e-05\n",
      "lr: 1.43e-05\n",
      "lr: 1.44e-05\n",
      "lr: 1.45e-05\n",
      "lr: 1.46e-05\n",
      "lr: 1.47e-05\n",
      "lr: 1.48e-05\n",
      "lr: 1.49e-05\n",
      "lr: 1.50e-05\n",
      "lr: 1.51e-05\n",
      "lr: 1.52e-05\n",
      "lr: 1.53e-05\n",
      "lr: 1.54e-05\n",
      "lr: 1.55e-05\n",
      "lr: 1.56e-05\n",
      "lr: 1.57e-05\n",
      "lr: 1.58e-05\n",
      "lr: 1.59e-05\n",
      "lr: 1.60e-05\n",
      "lr: 1.61e-05\n",
      "lr: 1.62e-05\n",
      "lr: 1.63e-05\n",
      "lr: 1.64e-05\n",
      "lr: 1.65e-05\n",
      "lr: 1.66e-05\n",
      "lr: 1.67e-05\n",
      "lr: 1.68e-05\n",
      "lr: 1.69e-05\n",
      "lr: 1.70e-05\n",
      "lr: 1.71e-05\n",
      "lr: 1.72e-05\n",
      "lr: 1.73e-05\n",
      "lr: 1.74e-05\n",
      "lr: 1.75e-05\n",
      "lr: 1.76e-05\n",
      "lr: 1.77e-05\n",
      "lr: 1.78e-05\n",
      "lr: 1.79e-05\n",
      "lr: 1.80e-05\n",
      "lr: 1.81e-05\n",
      "lr: 1.82e-05\n",
      "lr: 1.83e-05\n",
      "lr: 1.84e-05\n",
      "lr: 1.85e-05\n",
      "lr: 1.86e-05\n",
      "lr: 1.87e-05\n",
      "lr: 1.88e-05\n",
      "lr: 1.89e-05\n",
      "lr: 1.90e-05\n",
      "lr: 1.91e-05\n",
      "lr: 1.92e-05\n",
      "lr: 1.93e-05\n",
      "lr: 1.94e-05\n",
      "lr: 1.95e-05\n",
      "lr: 1.96e-05\n",
      "lr: 1.97e-05\n",
      "lr: 1.98e-05\n",
      "lr: 1.99e-05\n",
      "lr: 2.00e-05\n",
      "lr: 2.01e-05\n",
      "lr: 2.02e-05\n",
      "lr: 2.03e-05\n",
      "lr: 2.04e-05\n",
      "lr: 2.05e-05\n",
      "lr: 2.06e-05\n",
      "lr: 2.07e-05\n",
      "lr: 2.08e-05\n",
      "lr: 2.09e-05\n",
      "lr: 2.10e-05\n",
      "lr: 2.11e-05\n",
      "lr: 2.12e-05\n",
      "lr: 2.13e-05\n",
      "lr: 2.14e-05\n",
      "lr: 2.15e-05\n",
      "lr: 2.16e-05\n",
      "lr: 2.17e-05\n",
      "lr: 2.18e-05\n",
      "lr: 2.19e-05\n",
      "lr: 2.20e-05\n",
      "lr: 2.21e-05\n",
      "lr: 2.22e-05\n",
      "lr: 2.23e-05\n",
      "lr: 2.24e-05\n",
      "lr: 2.25e-05\n",
      "lr: 2.26e-05\n",
      "lr: 2.27e-05\n",
      "lr: 2.28e-05\n",
      "lr: 2.29e-05\n",
      "lr: 2.30e-05\n",
      "lr: 2.31e-05\n",
      "lr: 2.32e-05\n",
      "lr: 2.33e-05\n",
      "lr: 2.34e-05\n",
      "lr: 2.35e-05\n",
      "lr: 2.36e-05\n",
      "lr: 2.37e-05\n",
      "lr: 2.38e-05\n",
      "lr: 2.39e-05\n",
      "lr: 2.40e-05\n",
      "lr: 2.41e-05\n",
      "lr: 2.42e-05\n",
      "lr: 2.43e-05\n",
      "lr: 2.44e-05\n",
      "lr: 2.45e-05\n",
      "lr: 2.46e-05\n",
      "lr: 2.47e-05\n",
      "lr: 2.48e-05\n",
      "lr: 2.49e-05\n",
      "lr: 2.50e-05\n",
      "it: 250\tloss: 49.0535\tmiou-train: 0.284 \tmiou-val: 0.271\n",
      "lr: 2.51e-05\n",
      "lr: 2.52e-05\n",
      "lr: 2.53e-05\n",
      "lr: 2.54e-05\n",
      "lr: 2.55e-05\n",
      "lr: 2.56e-05\n",
      "lr: 2.57e-05\n",
      "lr: 2.58e-05\n",
      "lr: 2.59e-05\n",
      "lr: 2.60e-05\n",
      "lr: 2.61e-05\n",
      "lr: 2.62e-05\n",
      "lr: 2.63e-05\n",
      "lr: 2.64e-05\n",
      "lr: 2.65e-05\n",
      "lr: 2.66e-05\n",
      "lr: 2.67e-05\n",
      "lr: 2.68e-05\n",
      "lr: 2.69e-05\n",
      "lr: 2.70e-05\n",
      "lr: 2.71e-05\n",
      "lr: 2.72e-05\n",
      "lr: 2.73e-05\n",
      "lr: 2.74e-05\n",
      "lr: 2.75e-05\n",
      "lr: 2.76e-05\n",
      "lr: 2.77e-05\n",
      "lr: 2.78e-05\n",
      "lr: 2.79e-05\n",
      "lr: 2.80e-05\n",
      "lr: 2.81e-05\n",
      "lr: 2.82e-05\n",
      "lr: 2.83e-05\n",
      "lr: 2.84e-05\n",
      "lr: 2.85e-05\n",
      "lr: 2.86e-05\n",
      "lr: 2.87e-05\n",
      "lr: 2.88e-05\n",
      "lr: 2.89e-05\n",
      "lr: 2.90e-05\n",
      "lr: 2.91e-05\n",
      "lr: 2.92e-05\n",
      "lr: 2.93e-05\n",
      "lr: 2.94e-05\n",
      "lr: 2.95e-05\n",
      "lr: 2.96e-05\n",
      "lr: 2.97e-05\n",
      "lr: 2.98e-05\n",
      "lr: 2.99e-05\n",
      "lr: 3.00e-05\n",
      "lr: 3.01e-05\n",
      "lr: 3.02e-05\n",
      "lr: 3.03e-05\n",
      "lr: 3.04e-05\n",
      "lr: 3.05e-05\n",
      "lr: 3.06e-05\n",
      "lr: 3.07e-05\n",
      "lr: 3.08e-05\n",
      "lr: 3.09e-05\n",
      "lr: 3.10e-05\n",
      "lr: 3.11e-05\n",
      "lr: 3.12e-05\n",
      "lr: 3.13e-05\n",
      "lr: 3.14e-05\n",
      "lr: 3.15e-05\n",
      "lr: 3.16e-05\n",
      "lr: 3.17e-05\n",
      "lr: 3.18e-05\n",
      "lr: 3.19e-05\n",
      "lr: 3.20e-05\n",
      "lr: 3.21e-05\n",
      "lr: 3.22e-05\n",
      "lr: 3.23e-05\n",
      "lr: 3.24e-05\n",
      "lr: 3.25e-05\n",
      "lr: 3.26e-05\n",
      "lr: 3.27e-05\n",
      "lr: 3.28e-05\n",
      "lr: 3.29e-05\n",
      "lr: 3.30e-05\n",
      "lr: 3.31e-05\n",
      "lr: 3.32e-05\n",
      "lr: 3.33e-05\n",
      "lr: 3.34e-05\n",
      "lr: 3.35e-05\n",
      "lr: 3.36e-05\n",
      "lr: 3.37e-05\n",
      "lr: 3.38e-05\n",
      "lr: 3.39e-05\n",
      "lr: 3.40e-05\n",
      "lr: 3.41e-05\n",
      "lr: 3.42e-05\n",
      "lr: 3.43e-05\n",
      "lr: 3.44e-05\n",
      "lr: 3.45e-05\n",
      "lr: 3.46e-05\n",
      "lr: 3.47e-05\n",
      "lr: 3.48e-05\n",
      "lr: 3.49e-05\n",
      "lr: 3.50e-05\n",
      "lr: 3.51e-05\n",
      "lr: 3.52e-05\n",
      "lr: 3.53e-05\n",
      "lr: 3.54e-05\n",
      "lr: 3.55e-05\n",
      "lr: 3.56e-05\n",
      "lr: 3.57e-05\n",
      "lr: 3.58e-05\n",
      "lr: 3.59e-05\n",
      "lr: 3.60e-05\n",
      "lr: 3.61e-05\n",
      "lr: 3.62e-05\n",
      "lr: 3.63e-05\n",
      "lr: 3.64e-05\n",
      "lr: 3.65e-05\n",
      "lr: 3.66e-05\n",
      "lr: 3.67e-05\n",
      "lr: 3.68e-05\n",
      "lr: 3.69e-05\n",
      "lr: 3.70e-05\n",
      "lr: 3.71e-05\n",
      "lr: 3.72e-05\n",
      "lr: 3.73e-05\n",
      "lr: 3.74e-05\n",
      "lr: 3.75e-05\n",
      "lr: 3.76e-05\n",
      "lr: 3.77e-05\n",
      "lr: 3.78e-05\n",
      "lr: 3.79e-05\n",
      "lr: 3.80e-05\n",
      "lr: 3.81e-05\n",
      "lr: 3.82e-05\n",
      "lr: 3.83e-05\n",
      "lr: 3.84e-05\n",
      "lr: 3.85e-05\n",
      "lr: 3.86e-05\n",
      "lr: 3.87e-05\n",
      "lr: 3.88e-05\n",
      "lr: 3.89e-05\n",
      "lr: 3.90e-05\n",
      "lr: 3.91e-05\n",
      "lr: 3.92e-05\n",
      "lr: 3.93e-05\n",
      "lr: 3.94e-05\n",
      "lr: 3.95e-05\n",
      "lr: 3.96e-05\n",
      "lr: 3.97e-05\n",
      "lr: 3.98e-05\n",
      "lr: 3.99e-05\n",
      "lr: 4.00e-05\n",
      "lr: 4.01e-05\n",
      "lr: 4.02e-05\n",
      "lr: 4.03e-05\n",
      "lr: 4.04e-05\n",
      "lr: 4.05e-05\n",
      "lr: 4.06e-05\n",
      "lr: 4.07e-05\n",
      "lr: 4.08e-05\n",
      "lr: 4.09e-05\n",
      "lr: 4.10e-05\n",
      "lr: 4.11e-05\n",
      "lr: 4.12e-05\n",
      "lr: 4.13e-05\n",
      "lr: 4.14e-05\n",
      "lr: 4.15e-05\n",
      "lr: 4.16e-05\n",
      "lr: 4.17e-05\n",
      "lr: 4.18e-05\n",
      "lr: 4.19e-05\n",
      "lr: 4.20e-05\n",
      "lr: 4.21e-05\n",
      "lr: 4.22e-05\n",
      "lr: 4.23e-05\n",
      "lr: 4.24e-05\n",
      "lr: 4.25e-05\n",
      "lr: 4.26e-05\n",
      "lr: 4.27e-05\n",
      "lr: 4.28e-05\n",
      "lr: 4.29e-05\n",
      "lr: 4.30e-05\n",
      "lr: 4.31e-05\n",
      "lr: 4.32e-05\n",
      "lr: 4.33e-05\n",
      "lr: 4.34e-05\n",
      "lr: 4.35e-05\n",
      "lr: 4.36e-05\n",
      "lr: 4.37e-05\n",
      "lr: 4.38e-05\n",
      "lr: 4.39e-05\n",
      "lr: 4.40e-05\n",
      "lr: 4.41e-05\n",
      "lr: 4.42e-05\n",
      "lr: 4.43e-05\n",
      "lr: 4.44e-05\n",
      "lr: 4.45e-05\n",
      "lr: 4.46e-05\n",
      "lr: 4.47e-05\n",
      "lr: 4.48e-05\n",
      "lr: 4.49e-05\n",
      "lr: 4.50e-05\n",
      "lr: 4.51e-05\n",
      "lr: 4.52e-05\n",
      "lr: 4.53e-05\n",
      "lr: 4.54e-05\n",
      "lr: 4.55e-05\n",
      "lr: 4.56e-05\n",
      "lr: 4.57e-05\n",
      "lr: 4.58e-05\n",
      "lr: 4.59e-05\n",
      "lr: 4.60e-05\n",
      "lr: 4.61e-05\n",
      "lr: 4.62e-05\n",
      "lr: 4.63e-05\n",
      "lr: 4.64e-05\n",
      "lr: 4.65e-05\n",
      "lr: 4.66e-05\n",
      "lr: 4.67e-05\n",
      "lr: 4.68e-05\n",
      "lr: 4.69e-05\n",
      "lr: 4.70e-05\n",
      "lr: 4.71e-05\n",
      "lr: 4.72e-05\n",
      "lr: 4.73e-05\n",
      "lr: 4.74e-05\n",
      "lr: 4.75e-05\n",
      "lr: 4.76e-05\n",
      "lr: 4.77e-05\n",
      "lr: 4.78e-05\n",
      "lr: 4.79e-05\n",
      "lr: 4.80e-05\n",
      "lr: 4.81e-05\n",
      "lr: 4.82e-05\n",
      "lr: 4.83e-05\n",
      "lr: 4.84e-05\n",
      "lr: 4.85e-05\n",
      "lr: 4.86e-05\n",
      "lr: 4.87e-05\n",
      "lr: 4.88e-05\n",
      "lr: 4.89e-05\n",
      "lr: 4.90e-05\n",
      "lr: 4.91e-05\n",
      "lr: 4.92e-05\n",
      "lr: 4.93e-05\n",
      "lr: 4.94e-05\n",
      "lr: 4.95e-05\n",
      "lr: 4.96e-05\n",
      "lr: 4.97e-05\n",
      "lr: 4.98e-05\n",
      "lr: 4.99e-05\n",
      "lr: 5.00e-05\n",
      "it: 500\tloss: 14.5405\tmiou-train: 0.450 \tmiou-val: 0.419\n",
      "lr: 5.01e-05\n",
      "lr: 5.01e-05\n",
      "lr: 5.02e-05\n",
      "lr: 5.03e-05\n",
      "lr: 5.04e-05\n",
      "lr: 5.05e-05\n",
      "lr: 5.06e-05\n",
      "lr: 5.07e-05\n",
      "lr: 5.08e-05\n",
      "lr: 5.09e-05\n",
      "lr: 5.10e-05\n",
      "lr: 5.11e-05\n",
      "lr: 5.12e-05\n",
      "lr: 5.13e-05\n",
      "lr: 5.14e-05\n",
      "lr: 5.15e-05\n",
      "lr: 5.16e-05\n",
      "lr: 5.17e-05\n",
      "lr: 5.18e-05\n",
      "lr: 5.19e-05\n",
      "lr: 5.20e-05\n",
      "lr: 5.21e-05\n",
      "lr: 5.22e-05\n",
      "lr: 5.23e-05\n",
      "lr: 5.24e-05\n",
      "lr: 5.25e-05\n",
      "lr: 5.26e-05\n",
      "lr: 5.27e-05\n",
      "lr: 5.28e-05\n",
      "lr: 5.29e-05\n",
      "lr: 5.30e-05\n",
      "lr: 5.31e-05\n",
      "lr: 5.32e-05\n",
      "lr: 5.33e-05\n",
      "lr: 5.34e-05\n",
      "lr: 5.35e-05\n",
      "lr: 5.36e-05\n",
      "lr: 5.37e-05\n",
      "lr: 5.38e-05\n",
      "lr: 5.39e-05\n",
      "lr: 5.40e-05\n",
      "lr: 5.41e-05\n",
      "lr: 5.42e-05\n",
      "lr: 5.43e-05\n",
      "lr: 5.44e-05\n",
      "lr: 5.45e-05\n",
      "lr: 5.46e-05\n",
      "lr: 5.47e-05\n",
      "lr: 5.48e-05\n",
      "lr: 5.49e-05\n",
      "lr: 5.50e-05\n",
      "lr: 5.51e-05\n",
      "lr: 5.52e-05\n",
      "lr: 5.53e-05\n",
      "lr: 5.54e-05\n",
      "lr: 5.55e-05\n",
      "lr: 5.56e-05\n",
      "lr: 5.57e-05\n",
      "lr: 5.58e-05\n",
      "lr: 5.59e-05\n",
      "lr: 5.60e-05\n",
      "lr: 5.61e-05\n",
      "lr: 5.62e-05\n",
      "lr: 5.63e-05\n",
      "lr: 5.64e-05\n",
      "lr: 5.65e-05\n",
      "lr: 5.66e-05\n",
      "lr: 5.67e-05\n",
      "lr: 5.68e-05\n",
      "lr: 5.69e-05\n",
      "lr: 5.70e-05\n",
      "lr: 5.71e-05\n",
      "lr: 5.72e-05\n",
      "lr: 5.73e-05\n",
      "lr: 5.74e-05\n",
      "lr: 5.75e-05\n",
      "lr: 5.76e-05\n",
      "lr: 5.77e-05\n",
      "lr: 5.78e-05\n",
      "lr: 5.79e-05\n",
      "lr: 5.80e-05\n",
      "lr: 5.81e-05\n",
      "lr: 5.82e-05\n",
      "lr: 5.83e-05\n",
      "lr: 5.84e-05\n",
      "lr: 5.85e-05\n",
      "lr: 5.86e-05\n",
      "lr: 5.87e-05\n",
      "lr: 5.88e-05\n",
      "lr: 5.89e-05\n",
      "lr: 5.90e-05\n",
      "lr: 5.91e-05\n",
      "lr: 5.92e-05\n",
      "lr: 5.93e-05\n",
      "lr: 5.94e-05\n",
      "lr: 5.95e-05\n",
      "lr: 5.96e-05\n",
      "lr: 5.97e-05\n",
      "lr: 5.98e-05\n",
      "lr: 5.99e-05\n",
      "lr: 6.00e-05\n",
      "lr: 6.01e-05\n",
      "lr: 6.02e-05\n",
      "lr: 6.03e-05\n",
      "lr: 6.04e-05\n",
      "lr: 6.05e-05\n",
      "lr: 6.06e-05\n",
      "lr: 6.07e-05\n",
      "lr: 6.08e-05\n",
      "lr: 6.09e-05\n",
      "lr: 6.10e-05\n",
      "lr: 6.11e-05\n",
      "lr: 6.12e-05\n",
      "lr: 6.13e-05\n",
      "lr: 6.14e-05\n",
      "lr: 6.15e-05\n",
      "lr: 6.16e-05\n",
      "lr: 6.17e-05\n",
      "lr: 6.18e-05\n",
      "lr: 6.19e-05\n",
      "lr: 6.20e-05\n",
      "lr: 6.21e-05\n",
      "lr: 6.22e-05\n",
      "lr: 6.23e-05\n",
      "lr: 6.24e-05\n",
      "lr: 6.25e-05\n",
      "lr: 6.26e-05\n",
      "lr: 6.27e-05\n",
      "lr: 6.28e-05\n",
      "lr: 6.29e-05\n",
      "lr: 6.30e-05\n",
      "lr: 6.31e-05\n",
      "lr: 6.32e-05\n",
      "lr: 6.33e-05\n",
      "lr: 6.34e-05\n",
      "lr: 6.35e-05\n",
      "lr: 6.36e-05\n",
      "lr: 6.37e-05\n",
      "lr: 6.38e-05\n",
      "lr: 6.39e-05\n",
      "lr: 6.40e-05\n",
      "lr: 6.41e-05\n",
      "lr: 6.42e-05\n",
      "lr: 6.43e-05\n",
      "lr: 6.44e-05\n",
      "lr: 6.45e-05\n",
      "lr: 6.46e-05\n",
      "lr: 6.47e-05\n",
      "lr: 6.48e-05\n",
      "lr: 6.49e-05\n",
      "lr: 6.50e-05\n",
      "lr: 6.51e-05\n",
      "lr: 6.52e-05\n",
      "lr: 6.53e-05\n",
      "lr: 6.54e-05\n",
      "lr: 6.55e-05\n",
      "lr: 6.56e-05\n",
      "lr: 6.57e-05\n",
      "lr: 6.58e-05\n",
      "lr: 6.59e-05\n",
      "lr: 6.60e-05\n",
      "lr: 6.61e-05\n",
      "lr: 6.62e-05\n",
      "lr: 6.63e-05\n",
      "lr: 6.64e-05\n",
      "lr: 6.65e-05\n",
      "lr: 6.66e-05\n",
      "lr: 6.67e-05\n",
      "lr: 6.68e-05\n",
      "lr: 6.69e-05\n",
      "lr: 6.70e-05\n",
      "lr: 6.71e-05\n",
      "lr: 6.72e-05\n",
      "lr: 6.73e-05\n",
      "lr: 6.74e-05\n",
      "lr: 6.75e-05\n",
      "lr: 6.76e-05\n",
      "lr: 6.77e-05\n",
      "lr: 6.78e-05\n",
      "lr: 6.79e-05\n",
      "lr: 6.80e-05\n",
      "lr: 6.81e-05\n",
      "lr: 6.82e-05\n",
      "lr: 6.83e-05\n",
      "lr: 6.84e-05\n",
      "lr: 6.85e-05\n",
      "lr: 6.86e-05\n",
      "lr: 6.87e-05\n",
      "lr: 6.88e-05\n",
      "lr: 6.89e-05\n",
      "lr: 6.90e-05\n",
      "lr: 6.91e-05\n",
      "lr: 6.92e-05\n",
      "lr: 6.93e-05\n",
      "lr: 6.94e-05\n",
      "lr: 6.95e-05\n",
      "lr: 6.96e-05\n",
      "lr: 6.97e-05\n",
      "lr: 6.98e-05\n",
      "lr: 6.99e-05\n",
      "lr: 7.00e-05\n",
      "lr: 7.01e-05\n",
      "lr: 7.02e-05\n",
      "lr: 7.03e-05\n",
      "lr: 7.04e-05\n",
      "lr: 7.05e-05\n",
      "lr: 7.06e-05\n",
      "lr: 7.07e-05\n",
      "lr: 7.08e-05\n",
      "lr: 7.09e-05\n",
      "lr: 7.10e-05\n",
      "lr: 7.11e-05\n",
      "lr: 7.12e-05\n",
      "lr: 7.13e-05\n",
      "lr: 7.14e-05\n",
      "lr: 7.15e-05\n",
      "lr: 7.16e-05\n",
      "lr: 7.17e-05\n",
      "lr: 7.18e-05\n",
      "lr: 7.19e-05\n",
      "lr: 7.20e-05\n",
      "lr: 7.21e-05\n",
      "lr: 7.22e-05\n",
      "lr: 7.23e-05\n",
      "lr: 7.24e-05\n",
      "lr: 7.25e-05\n",
      "lr: 7.26e-05\n",
      "lr: 7.27e-05\n",
      "lr: 7.28e-05\n",
      "lr: 7.29e-05\n",
      "lr: 7.30e-05\n",
      "lr: 7.31e-05\n",
      "lr: 7.32e-05\n",
      "lr: 7.33e-05\n",
      "lr: 7.34e-05\n",
      "lr: 7.35e-05\n",
      "lr: 7.36e-05\n",
      "lr: 7.37e-05\n",
      "lr: 7.38e-05\n",
      "lr: 7.39e-05\n",
      "lr: 7.40e-05\n",
      "lr: 7.41e-05\n",
      "lr: 7.42e-05\n",
      "lr: 7.43e-05\n",
      "lr: 7.44e-05\n",
      "lr: 7.45e-05\n",
      "lr: 7.46e-05\n",
      "lr: 7.47e-05\n",
      "lr: 7.48e-05\n",
      "lr: 7.49e-05\n",
      "it: 750\tloss: 9.1594\tmiou-train: 0.522 \tmiou-val: 0.472\n",
      "lr: 7.50e-05\n",
      "lr: 7.51e-05\n",
      "lr: 7.52e-05\n",
      "lr: 7.53e-05\n",
      "lr: 7.54e-05\n",
      "lr: 7.55e-05\n",
      "lr: 7.56e-05\n",
      "lr: 7.57e-05\n",
      "lr: 7.58e-05\n",
      "lr: 7.59e-05\n",
      "lr: 7.60e-05\n",
      "lr: 7.61e-05\n",
      "lr: 7.62e-05\n",
      "lr: 7.63e-05\n",
      "lr: 7.64e-05\n",
      "lr: 7.65e-05\n",
      "lr: 7.66e-05\n",
      "lr: 7.67e-05\n",
      "lr: 7.68e-05\n",
      "lr: 7.69e-05\n",
      "lr: 7.70e-05\n",
      "lr: 7.71e-05\n",
      "lr: 7.72e-05\n",
      "lr: 7.73e-05\n",
      "lr: 7.74e-05\n",
      "lr: 7.75e-05\n",
      "lr: 7.76e-05\n",
      "lr: 7.77e-05\n",
      "lr: 7.78e-05\n",
      "lr: 7.79e-05\n",
      "lr: 7.80e-05\n",
      "lr: 7.81e-05\n",
      "lr: 7.82e-05\n",
      "lr: 7.83e-05\n",
      "lr: 7.84e-05\n",
      "lr: 7.85e-05\n",
      "lr: 7.86e-05\n",
      "lr: 7.87e-05\n",
      "lr: 7.88e-05\n",
      "lr: 7.89e-05\n",
      "lr: 7.90e-05\n",
      "lr: 7.91e-05\n",
      "lr: 7.92e-05\n",
      "lr: 7.93e-05\n",
      "lr: 7.94e-05\n",
      "lr: 7.95e-05\n",
      "lr: 7.96e-05\n",
      "lr: 7.97e-05\n",
      "lr: 7.98e-05\n",
      "lr: 7.99e-05\n",
      "lr: 8.00e-05\n",
      "lr: 8.01e-05\n",
      "lr: 8.02e-05\n",
      "lr: 8.03e-05\n",
      "lr: 8.04e-05\n",
      "lr: 8.05e-05\n",
      "lr: 8.06e-05\n",
      "lr: 8.07e-05\n",
      "lr: 8.08e-05\n",
      "lr: 8.09e-05\n",
      "lr: 8.10e-05\n",
      "lr: 8.11e-05\n",
      "lr: 8.12e-05\n",
      "lr: 8.13e-05\n",
      "lr: 8.14e-05\n",
      "lr: 8.15e-05\n",
      "lr: 8.16e-05\n",
      "lr: 8.17e-05\n",
      "lr: 8.18e-05\n",
      "lr: 8.19e-05\n",
      "lr: 8.20e-05\n",
      "lr: 8.21e-05\n",
      "lr: 8.22e-05\n",
      "lr: 8.23e-05\n",
      "lr: 8.24e-05\n",
      "lr: 8.25e-05\n",
      "lr: 8.26e-05\n",
      "lr: 8.27e-05\n",
      "lr: 8.28e-05\n",
      "lr: 8.29e-05\n",
      "lr: 8.30e-05\n",
      "lr: 8.31e-05\n",
      "lr: 8.32e-05\n",
      "lr: 8.33e-05\n",
      "lr: 8.34e-05\n",
      "lr: 8.35e-05\n",
      "lr: 8.36e-05\n",
      "lr: 8.37e-05\n",
      "lr: 8.38e-05\n",
      "lr: 8.39e-05\n",
      "lr: 8.40e-05\n",
      "lr: 8.41e-05\n",
      "lr: 8.42e-05\n",
      "lr: 8.43e-05\n",
      "lr: 8.44e-05\n",
      "lr: 8.45e-05\n",
      "lr: 8.46e-05\n",
      "lr: 8.47e-05\n",
      "lr: 8.48e-05\n",
      "lr: 8.49e-05\n",
      "lr: 8.50e-05\n",
      "lr: 8.51e-05\n",
      "lr: 8.52e-05\n",
      "lr: 8.53e-05\n",
      "lr: 8.54e-05\n",
      "lr: 8.55e-05\n",
      "lr: 8.56e-05\n",
      "lr: 8.57e-05\n",
      "lr: 8.58e-05\n",
      "lr: 8.59e-05\n",
      "lr: 8.60e-05\n",
      "lr: 8.61e-05\n",
      "lr: 8.62e-05\n",
      "lr: 8.63e-05\n",
      "lr: 8.64e-05\n",
      "lr: 8.65e-05\n",
      "lr: 8.66e-05\n",
      "lr: 8.67e-05\n",
      "lr: 8.68e-05\n",
      "lr: 8.69e-05\n",
      "lr: 8.70e-05\n",
      "lr: 8.71e-05\n",
      "lr: 8.72e-05\n",
      "lr: 8.73e-05\n",
      "lr: 8.74e-05\n",
      "lr: 8.75e-05\n",
      "lr: 8.76e-05\n",
      "lr: 8.77e-05\n",
      "lr: 8.78e-05\n",
      "lr: 8.79e-05\n",
      "lr: 8.80e-05\n",
      "lr: 8.81e-05\n",
      "lr: 8.82e-05\n",
      "lr: 8.83e-05\n",
      "lr: 8.84e-05\n",
      "lr: 8.85e-05\n",
      "lr: 8.86e-05\n",
      "lr: 8.87e-05\n",
      "lr: 8.88e-05\n",
      "lr: 8.89e-05\n",
      "lr: 8.90e-05\n",
      "lr: 8.91e-05\n",
      "lr: 8.92e-05\n",
      "lr: 8.93e-05\n",
      "lr: 8.94e-05\n",
      "lr: 8.95e-05\n",
      "lr: 8.96e-05\n",
      "lr: 8.97e-05\n",
      "lr: 8.98e-05\n",
      "lr: 8.99e-05\n",
      "lr: 9.00e-05\n",
      "lr: 9.01e-05\n",
      "lr: 9.02e-05\n",
      "lr: 9.03e-05\n",
      "lr: 9.04e-05\n",
      "lr: 9.05e-05\n",
      "lr: 9.06e-05\n",
      "lr: 9.07e-05\n",
      "lr: 9.08e-05\n",
      "lr: 9.09e-05\n",
      "lr: 9.10e-05\n",
      "lr: 9.11e-05\n",
      "lr: 9.12e-05\n",
      "lr: 9.13e-05\n",
      "lr: 9.14e-05\n",
      "lr: 9.15e-05\n",
      "lr: 9.16e-05\n",
      "lr: 9.17e-05\n",
      "lr: 9.18e-05\n",
      "lr: 9.19e-05\n",
      "lr: 9.20e-05\n",
      "lr: 9.21e-05\n",
      "lr: 9.22e-05\n",
      "lr: 9.23e-05\n",
      "lr: 9.24e-05\n",
      "lr: 9.25e-05\n",
      "lr: 9.26e-05\n",
      "lr: 9.27e-05\n",
      "lr: 9.28e-05\n",
      "lr: 9.29e-05\n",
      "lr: 9.30e-05\n",
      "lr: 9.31e-05\n",
      "lr: 9.32e-05\n",
      "lr: 9.33e-05\n",
      "lr: 9.34e-05\n",
      "lr: 9.35e-05\n",
      "lr: 9.36e-05\n",
      "lr: 9.37e-05\n",
      "lr: 9.38e-05\n",
      "lr: 9.39e-05\n",
      "lr: 9.40e-05\n",
      "lr: 9.41e-05\n",
      "lr: 9.42e-05\n",
      "lr: 9.43e-05\n",
      "lr: 9.44e-05\n",
      "lr: 9.45e-05\n",
      "lr: 9.46e-05\n",
      "lr: 9.47e-05\n",
      "lr: 9.48e-05\n",
      "lr: 9.49e-05\n",
      "lr: 9.50e-05\n",
      "lr: 9.51e-05\n",
      "lr: 9.52e-05\n",
      "lr: 9.53e-05\n",
      "lr: 9.54e-05\n",
      "lr: 9.55e-05\n",
      "lr: 9.56e-05\n",
      "lr: 9.57e-05\n",
      "lr: 9.58e-05\n",
      "lr: 9.59e-05\n",
      "lr: 9.60e-05\n",
      "lr: 9.61e-05\n",
      "lr: 9.62e-05\n",
      "lr: 9.63e-05\n",
      "lr: 9.64e-05\n",
      "lr: 9.65e-05\n",
      "lr: 9.66e-05\n",
      "lr: 9.67e-05\n",
      "lr: 9.68e-05\n",
      "lr: 9.69e-05\n",
      "lr: 9.70e-05\n",
      "lr: 9.71e-05\n",
      "lr: 9.72e-05\n",
      "lr: 9.73e-05\n",
      "lr: 9.74e-05\n",
      "lr: 9.75e-05\n",
      "lr: 9.76e-05\n",
      "lr: 9.77e-05\n",
      "lr: 9.78e-05\n",
      "lr: 9.79e-05\n",
      "lr: 9.80e-05\n",
      "lr: 9.81e-05\n",
      "lr: 9.82e-05\n",
      "lr: 9.83e-05\n",
      "lr: 9.84e-05\n",
      "lr: 9.85e-05\n",
      "lr: 9.86e-05\n",
      "lr: 9.87e-05\n",
      "lr: 9.88e-05\n",
      "lr: 9.89e-05\n",
      "lr: 9.90e-05\n",
      "lr: 9.91e-05\n",
      "lr: 9.92e-05\n",
      "lr: 9.93e-05\n",
      "lr: 9.94e-05\n",
      "lr: 9.95e-05\n",
      "lr: 9.96e-05\n",
      "lr: 9.97e-05\n",
      "lr: 9.98e-05\n",
      "lr: 9.99e-05\n",
      "it: 1000\tloss: 7.1536\tmiou-train: 0.565 \tmiou-val: 0.494\n",
      "lr: 1.00e-04\n",
      "it: 1250\tloss: 5.9255\tmiou-train: 0.646 \tmiou-val: 0.513\n",
      "it: 1500\tloss: 5.0766\tmiou-train: 0.658 \tmiou-val: 0.502\n",
      "it: 1750\tloss: 4.6735\tmiou-train: 0.640 \tmiou-val: 0.505\n",
      "it: 2000\tloss: 4.4113\tmiou-train: 0.668 \tmiou-val: 0.514\n",
      "it: 2250\tloss: 4.2585\tmiou-train: 0.647 \tmiou-val: 0.477\n",
      "it: 2500\tloss: 4.0301\tmiou-train: 0.673 \tmiou-val: 0.510\n",
      "it: 2750\tloss: 3.8712\tmiou-train: 0.676 \tmiou-val: 0.527\n",
      "it: 3000\tloss: 3.5689\tmiou-train: 0.694 \tmiou-val: 0.500\n",
      "it: 3250\tloss: 3.4370\tmiou-train: 0.699 \tmiou-val: 0.547\n",
      "it: 3500\tloss: 3.3724\tmiou-train: 0.712 \tmiou-val: 0.531\n",
      "it: 3750\tloss: 3.3478\tmiou-train: 0.704 \tmiou-val: 0.517\n",
      "it: 4000\tloss: 3.3276\tmiou-train: 0.702 \tmiou-val: 0.513\n",
      "it: 4250\tloss: 3.1838\tmiou-train: 0.707 \tmiou-val: 0.526\n",
      "it: 4500\tloss: 3.2369\tmiou-train: 0.714 \tmiou-val: 0.521\n",
      "it: 4750\tloss: 3.0458\tmiou-train: 0.715 \tmiou-val: 0.545\n",
      "it: 5000\tloss: 3.0045\tmiou-train: 0.718 \tmiou-val: 0.535\n",
      "it: 5250\tloss: 2.7217\tmiou-train: 0.736 \tmiou-val: 0.534\n",
      "it: 5500\tloss: 2.6157\tmiou-train: 0.730 \tmiou-val: 0.541\n",
      "it: 5750\tloss: 2.5157\tmiou-train: 0.735 \tmiou-val: 0.538\n",
      "it: 6000\tloss: 2.5200\tmiou-train: 0.736 \tmiou-val: 0.534\n",
      "it: 6250\tloss: 2.4673\tmiou-train: 0.737 \tmiou-val: 0.528\n",
      "it: 6500\tloss: 2.4518\tmiou-train: 0.741 \tmiou-val: 0.530\n",
      "it: 6750\tloss: 2.4400\tmiou-train: 0.749 \tmiou-val: 0.524\n",
      "it: 7000\tloss: 2.4240\tmiou-train: 0.735 \tmiou-val: 0.526\n",
      "it: 7250\tloss: 2.3916\tmiou-train: 0.739 \tmiou-val: 0.528\n",
      "it: 7500\tloss: 2.3603\tmiou-train: 0.732 \tmiou-val: 0.521\n",
      "it: 7750\tloss: 2.3594\tmiou-train: 0.741 \tmiou-val: 0.510\n",
      "it: 8000\tloss: 2.3864\tmiou-train: 0.752 \tmiou-val: 0.519\n",
      "it: 8250\tloss: 2.3452\tmiou-train: 0.749 \tmiou-val: 0.512\n",
      "it: 8500\tloss: 2.3416\tmiou-train: 0.738 \tmiou-val: 0.510\n",
      "it: 8750\tloss: 2.2862\tmiou-train: 0.744 \tmiou-val: 0.515\n",
      "it: 9000\tloss: 2.2847\tmiou-train: 0.742 \tmiou-val: 0.515\n",
      "it: 9250\tloss: 2.2424\tmiou-train: 0.752 \tmiou-val: 0.513\n",
      "it: 9500\tloss: 2.2190\tmiou-train: 0.742 \tmiou-val: 0.510\n",
      "it: 9750\tloss: 2.2182\tmiou-train: 0.743 \tmiou-val: 0.511\n",
      "it: 10000\tloss: 2.2496\tmiou-train: 0.746 \tmiou-val: 0.512\n"
     ]
    }
   ],
   "source": [
    "fl_resume = True\n",
    "fl_resume = False\n",
    "\n",
    "lossi = []\n",
    "losses = []\n",
    "if not fl_resume:\n",
    "    it = 1\n",
    "    miou_best = 0\n",
    "    steps_cur = 0    \n",
    "       \n",
    "    lr_cur = opt.param_groups[0][\"lr\"]\n",
    "    print(f'lr: {lr_cur:.2e}')\n",
    "    writer.add_scalars('lr', {'nn': lr_cur}, steps_cur)\n",
    "        \n",
    "    model.eval()\n",
    "    miou_train = ut.get_CM_fromloader(train_loader, model, n_classes)[0]\n",
    "    miou_val = ut.get_CM_fromloader(val_loader, model, n_classes)[0]\n",
    "    print(f'\\tmiou-train: {miou_train:.2f}\\tmiou-val: {miou_val:.2f}')\n",
    "    writer.add_scalars('miou', {'train': miou_train}, steps_cur)\n",
    "    writer.add_scalars('miou', {'val': miou_val}, steps_cur)\n",
    "\n",
    "\n",
    "print_every_n = 250\n",
    "iters_total = 10000\n",
    "n_gradacc = max(8//bs_train,1) # bs=8\n",
    "iters_total *= n_gradacc\n",
    "print_every_n *= n_gradacc\n",
    "print(f'grad-acc: {n_gradacc}')\n",
    "print(f'iters_total: {iters_total}')\n",
    "steps_cur = it // print_every_n\n",
    "\n",
    "model.train()\n",
    "opt.zero_grad(set_to_none=True)\n",
    "while it <= iters_total:\n",
    "    \n",
    "    for inp_im, inp_label in train_loader:\n",
    "        inp_im = inp_im.cuda()\n",
    "        inp_label = inp_label[:, 0].cuda()\n",
    "\n",
    "        # Only forward pass goes into the autocast context\n",
    "        with torch.amp.autocast(dtype=torch.bfloat16, enabled=True, device_type='cuda'):\n",
    "            segmentation_map, loss = model(inp_im, inp_label)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        # Accumulate gradients\n",
    "        if it % n_gradacc == 0:\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)            \n",
    "        \n",
    "\n",
    "        lossi.append(loss.item())\n",
    "        it += 1\n",
    "\n",
    "        if scheduler.fl_warmup and it % warmup_step == 0:\n",
    "            scheduler.step()\n",
    "            print(f'lr: {opt.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        if it % print_every_n == 0:\n",
    "            steps_cur += 1\n",
    "\n",
    "            loss_avg = np.mean(lossi)\n",
    "            losses.append(loss_avg)\n",
    "\n",
    "            model.eval()\n",
    "            miou_train = ut.get_CM_fromloader(train_loader, model, n_classes)[0]\n",
    "            miou_val = ut.get_CM_fromloader(val_loader, model, n_classes)[0]\n",
    "            model.train()\n",
    "\n",
    "            print(\n",
    "                f'it: {it}\\tloss: {loss_avg:.4f}'\n",
    "                f'\\tmiou-train: {miou_train:.3f}',\n",
    "                f'\\tmiou-val: {miou_val:.3f}'\n",
    "            )\n",
    "\n",
    "            writer.add_scalars('objective', {'train': loss_avg}, steps_cur)\n",
    "            writer.add_scalars('miou', {'train': miou_train}, steps_cur)\n",
    "            writer.add_scalars('miou', {'val': miou_val}, steps_cur)\n",
    "\n",
    "            \n",
    "            lossi = []\n",
    "            \n",
    "            ckpt_last_path = os.path.join(logdir, 'model.last.pth')\n",
    "            ut.save_ckpt(ckpt_last_path, model, opt, miou_val, it)\n",
    "\n",
    "            if miou_val > miou_best:\n",
    "                miou_best = miou_val\n",
    "                ckpt_last_path = os.path.join(logdir, 'model.best.pth')\n",
    "                ut.save_ckpt(ckpt_last_path, model, opt, miou_val, it)\n",
    "\n",
    "            if it in [5000, 9000]:\n",
    "                for pg in opt.param_groups:\n",
    "                    pg['lr'] *= .1\n",
    "                    \n",
    "        if it > iters_total:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cf5dcb-5dda-4b65-8de3-ee8346210ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%notify` not found.\n"
     ]
    }
   ],
   "source": [
    "%%notify -m \"Training finished ðŸŽ‰\"\n",
    "print('oi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7d575-a028-4151-b987-0a9d785a19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pg in opt.param_groups:\n",
    "#     print(pg['lr'])\n",
    "#     pg['lr'] *= .1\n",
    "#     print(pg['lr'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c957126-5a1c-465b-867f-f4115b915836",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(dummy_im.cuda())[0].cpu()\n",
    "ims = ut.Normalize.reverse(dummy_im)\n",
    "# labels = colorizer(dummy_label)\n",
    "labels = colorizer(pred)\n",
    "alpha = .2\n",
    "blend = (1-alpha)*ims + alpha*labels\n",
    "tmp = torch.concat([ims, labels, blend], axis=-2)\n",
    "tmp = tmp.moveaxis(0,-2).flatten(-2,-1).permute(1,2,0)\n",
    "tmp = ut.float_to_uint8(tmp.numpy())\n",
    "Image.fromarray(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e2939-69bb-4182-ae63-b5b987a00bf2",
   "metadata": {},
   "source": [
    "# Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaca2b0-7c0c-4416-b70d-508c237b86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f = ut.MyMask2Former.from_pretrained(\n",
    "            \"facebook/mask2former-swin-base-IN21k-ade-semantic\",\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "m2f.eval().cuda();\n",
    "\n",
    "\n",
    "modelpath = '../logs/20240702_1358-m2f-mocambav034+rtk-res1024/model.best.pth'\n",
    "state_dict = torch.load(modelpath)['state']\n",
    "m2f.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02920d0b-b8e7-4458-9875-dfaa4a639735",
   "metadata": {},
   "outputs": [],
   "source": [
    "miou, CM_iou, CM_abs = ut.get_CM_fromloader(val_loader, m2f, n_classes)\n",
    "print(miou.round(2))\n",
    "print(CM_iou.round(2))\n",
    "# v03.2\n",
    "# 0.72\n",
    "# [0.99  nan 0.92 0.23 0.3  0.88 0.9 0.6   nan 0.6  0.77 0.71 0.82 0.8 0.84 0.81 0.71]\n",
    "\n",
    "# v03\n",
    "# 0.68\n",
    "# [0.98  nan 0.91 0.16 0.24 0.63 0.81 0.61  nan 0.49 0.76 0.73 0.78 0.79\n",
    "#  0.86 0.79 0.58]\n",
    "\n",
    "# v02\n",
    "# 0.63\n",
    "# [0.99  nan 0.91 0.22 0.18 0.61 0.66 0.62  nan 0.41 0.71 0.71 0.7  0.37\n",
    "#  0.86 0.85 0.59]\n",
    "\n",
    "# v01\n",
    "# 0.69\n",
    "# array([0.99,  nan, 0.95, 0.48, 0.49, 0.7 , 0.73,  nan, 0.43, 0.04, 0.56,\n",
    "#        0.66, 0.88, 0.85, 0.55, 0.66, 0.97, 0.84, 0.67,  nan, 0.84, 0.77,\n",
    "#        0.7 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8490723-6aae-4391-a4a0-9396ded343b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mocamba_classnames = {\n",
    "#     0: 'background', 1: 'thing-Animals', 2: 'surface-Asphalt', 3: 'sign-Cat-s-Eye', 4: 'damage-Cracks', 5: 'thing-Ego',\n",
    "#     6: 'surface-Hard-Sand', 7: 'sign-Markings', 8: 'thing-Obstacle', 9: 'thing-People', 10: 'damage-Pothole', 11: 'thing-Retaining-wall',\n",
    "#     12: 'surface-Soft-Sand', 13: 'surface-Unpaved', 14: 'thing-Vehicles', 15: 'sign-Vertical-Signs', 16: 'surface-Wet-sand'\n",
    "# }\n",
    "\n",
    "\n",
    "txt = ''\n",
    "for v, p in zip(mocamba_classnames.values(), CM_iou):\n",
    "    txt += f'{v}:\\t{round(p*100,2)}\\n'\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8ba50-1ace-4481-9f58-ba26b6f15840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpath = 'all-trainpaths-wider.txt'\n",
    "# fpath = 'all-valpaths-wider.txt'\n",
    "fpath = '../data/mocamba/ds-mocamba-v0.3.3-long1024/trainpaths.txt'\n",
    "\n",
    "\n",
    "val_ds = ut.SimpleDataset(annotation_file=fpath)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "m2f.eval()\n",
    "alpha = .2\n",
    "nplot = 10\n",
    "\n",
    "plot_ims = []\n",
    "for inp_im, inp_label in val_loader:\n",
    "    pred_m2f = m2f(inp_im.cuda())[0].cpu()    \n",
    "    im = ut.Normalize.reverse(inp_im)\n",
    "    label = colorizer(inp_label)\n",
    "    \n",
    "    pred_m2f = colorizer(pred_m2f)\n",
    "    blend = (1-alpha)*im + alpha*pred_m2f\n",
    "\n",
    "    tmp = [im, label, pred_m2f, blend]\n",
    "    tmp = [F.pad(x, (0, 1024-x.shape[-1], 0, 0, 0, 0, 0, 0)) for x in tmp]\n",
    "\n",
    "    tmp = torch.concat([torch.concat(tmp, axis=-1)], axis=-2)\n",
    "    tmp = tmp.moveaxis(0,-2).flatten(-2,-1).permute(1,2,0)\n",
    "    tmp = ut.float_to_uint8(tmp.numpy())\n",
    "    plot_ims.append(tmp)\n",
    "\n",
    "    if len(plot_ims) >= nplot:\n",
    "        break\n",
    "\n",
    "\n",
    "tmp = np.concatenate(plot_ims, axis=0)\n",
    "Image.fromarray(tmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
